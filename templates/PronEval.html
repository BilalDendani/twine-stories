 
            <html> 
                <body> 
                <p>Choose one of the three options below, or click the record button and say the sentence "We drank tea in the afternoon and watched tv". Click the evaluate button to then find out the intelligibilty score of your speech recording. 

</p>
                <form action="http://localhost:5000/next"  method = "POST">
                   <p>Return to the previous page.
                    <button name="option" id="option1" type = "submit"  value="Start">Go</button> 
                       <p>Find out more about the intelligibility remediation. 
                    <button name="option" id="option2" type = "submit"  value="Info">Go</button> 

            </form>
        
<body>
<div class="container">
<link type="text/css" rel="stylesheet" href="static/css/materialize.min.css"  media="screen,projection"/>
<div class="row">
<div class="pad-top"></div>
</div>
<div class="row"  style="text-align: center;">
        <div class="col s12 m12">
          <div>
            <div class="card-content">
<div class="row">
<div class="col s1 offset-s9">
</div>


    <select id="grammars">Select</select>
 

    <a style="display: inline;">Press to: Record/Stop </a>
    <button id="startBtn" style= ><i class="fa fa-microphone" aria-hidden="true" id="icon"></i>
</button>
  
<!--       <audio id="recordedAudio" controls="controls">
      </audio><br/><br/> -->
          <span id="playing-indicator" ></span>
         <h10> <a style="display: inline;">Play </a></h10>
      <button  id="play" title="Play" ><i class="fa fa-play" aria-hidden="true"></i></button>
      <a style="display: inline;">Evaluate </a>    
      <button  id="eval" ><i class="fa fa-question" aria-hidden="true"></i></button>
      

<!-- The Modal -->

<div class="row"  style="text-align: center;">
        <div class="col s10 m10 offset-s1 offset-m1">
          <div class="card brown lighten-4">
            <div class="card-content">
            <div id="output" style="height:20px;overflow:auto;" >
            </div>
            </div>
          </div>
        </div>
    </div>    
    <div id="resultymf"></div>
    </div>
          </div>
        </div>
    </div>
</div>
</div>
</div>
</div>
</div>
         
    <script>
      // These will be initialized later
      var recognizer, recorder, callbackManager, audioContext, outputContainer,rec;
      // Only when both recorder and recognizer do we have a ready application
      var choice="ONE"; //To hold the choice of the user

      var isRecorderReady = isRecognizerReady = false;
      var decode_word = "drank";
      var outputSampleRate = 16000;
      var inSampleRate;
      // Intelligibility Score
      var int_score;
      // A convenience function to post a message to the recognizer and associate
      // a callback to its response
      function postRecognizerJob(message, callback) {
        var msg = message || {}; 
        if (callbackManager) msg.callbackId = callbackManager.add(callback);
        if (recognizer) recognizer.postMessage(msg);
      };
    //This function initializes an instance of the recorder
    //it posts a message right away and calls onReady when it
    //is ready so that onmessage can be properly set
      function spawnWorker(workerURL, onReady) {
          recognizer = new Worker(workerURL);
          recognizer.onmessage = function(event) {
            onReady(recognizer);
          };
          recognizer.postMessage('');
      };
      // To display the hypothesis sent by the recognizer
      function updateHyp(hyp) {
        if (outputContainer) outputContainer.innerHTML = hyp;

      };
      // This updates the UI when the app might get ready
      // Only when both recorder and recognizer are ready do we enable the buttons
      function updateUI() {
        if (isRecorderReady && isRecognizerReady) startBtn.disabled = false;
      };
      // This is just a logging window where we display the  
      function updateStatus(newStatus) {
        console.log("Status:" +newStatus)
      };
     // A not-so-great recording indicator
      function displayRecording(display) {
        // if (display) document.getElementById('recording-indicator').innerHTML = "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;";
        // else document.getElementById('recording-indicator').innerHTML = "";
      };

      var audioContext = new AudioContext(); 
      function startUserMedia(stream) {
        var input = audioContext.createMediaStreamSource(stream);
        // Firefox hack https://support.mozilla.org/en-US/questions/984179
        window.firefox_audio_hack = input;
        var audioRecorderConfig = {errorCallback: function(x) {updateStatus("Error from recorder: " + x);}};
        recorder = new AudioRecorder(input, audioRecorderConfig);
        rec = new Recorder(input);
        // If a recognizer is ready, we pass it to the recorder
        if (recognizer) recorder.consumers = [recognizer];
        isRecorderReady = true;
        updateUI();
        updateStatus("Audio recorder ready");
      };
          
  var playbackRecorderAudio = function (recorder, context) {
    recorder.getBuffer(function (buffers) {
      var source = context.createBufferSource();
      source.buffer = context.createBuffer(1, buffers[0].length, 44100);
      source.buffer.getChannelData(0).set(buffers[0]);
      source.connect(context.destination);

      source.start(0);
    });
  }
      var check = function(){
        var id = document.getElementById('icon')
        var img = id.getAttribute('class')
        if( img == "fa fa-microphone")
        {
          id.setAttribute("class","fa fa-stop");
          startRecording();
        }
        else
        {
          id.setAttribute("class","fa fa-microphone");
          stopRecording();

                                   
          //document.getElementById('startBtn').checked = false;
        }
      }
      // This starts recording. We first need to get the id of the grammar to use
       function startRecording() {
        var id = document.getElementById('grammars').value;
        if (recorder && recorder.start(id)) displayRecording(true)
        rec && rec.record();
      };
      // Stops recording
         function stopRecording() {

        recorder && recorder.stop();
        rec && rec.stop();
        displayRecording(false);
        updateHyp(choice);
      

    };

    var submit = function(){
   //     if(choice=="change the word recently to reference the date of the event")
   //            {
   //              //document.getElementById("option1").click();
   //              decode_word = "change the word recently to reference the date of the event"
   //            }
			// else if(choice=="delete the word recently and leave the rest as it it")
   //            {
   //             // document.getElementById("option2").click();
   //              decode_word = "delete the word recently and leave the rest as it it"
   //            }
			// else if(choice=="delete the claim because it does not have a source")
   //            {
   //              //document.getElementById("option3").click();
   //              decode_word = "delete the claim because it does not have a source"
   //            }

    };

    var feat = function(){
      context = audioContext
      var aud;
      var i16_buf;
      rec.getBuffer(function (buffers) {
      var source = context.createBufferSource();
      source.buffer = context.createBuffer(1, buffers[0].length, 44100);
      source.buffer.getChannelData(0).set(buffers[0]);
      source.connect(context.destination);
      buf = source.buffer.getChannelData(0);
      console.log(buf);
      var i16 = format_audio(buf);
      //console.log(i16);


      postRecognizerJob({command: 'featex', data: {array: i16, word: decode_word}});

    });
      

      //recorder.featex(decode_word);
    }

    function ajax_post(_url, features){
      console.log(features);
      $.ajax({
        type: 'POST',
        url: _url, 
        //data: '{ "feats": [0.04615384712815285, 0.15078403055667877, 0.9285714030265808, 0.9750000238418579, 0.0615384615957737, 0.1498793512582779, 0.1666666716337204, 0.40625, 0.5384615659713745, 0.1136680319905281, 0.8571428656578064, 0.33125001192092896, 0.04615384712815285, 0.16024932265281677, 0.738095223903656, 0.42500001192092896, 0.07692307978868484, 0.14431020617485046, 0.976190447807312, 0.7562500238418579, 0.24475], "word": "because" }',
         //data: '{ "feats": [0.04615384712815285, 0.14569181203842163,0.13846154510974884, 0.13181662559509277, 0.0923076942563057, 0.14237390458583832], "word":"because" }',
        data: features,
        dataType: 'json',
        contentType: 'application/json; charset=utf-8',
        success: function(score){
          console.log(score);
        }
                        }
        );
    }
      
      var playback = function(e){
          playbackRecorderAudio(rec, audioContext);
      }
      // Called once the recognizer is ready
      // We then add the grammars to the input select tag and update the UI
      var recognizerReady = function() {
           updateGrammars();
           isRecognizerReady = true;
           updateUI();
           updateStatus("Recognizer ready");
      };
      // We get the grammars defined below and fill in the input select tag
      var updateGrammars = function() {
        var selectTag = document.getElementById('grammars');
        for (var i = 0 ; i < grammarIds.length ; i++) {
            var newElt = document.createElement('option');
            newElt.value=grammarIds[i].id;
            newElt.innerHTML = grammarIds[i].title;
            selectTag.appendChild(newElt);
        }
      };
      // This adds a grammar from the grammars array
      // We add them one by one and call it again as
      // a callback.
      // Once we are done adding all grammars, we can call
      // recognizerReady()
      var feedGrammar = function(g, index, id) {
        if (id && (grammarIds.length > 0)) grammarIds[0].id = id.id;
        if (index < g.length) {
          grammarIds.unshift({title: g[index].title})
    postRecognizerJob({command: 'addGrammar', data: g[index].g},
                             function(id) {feedGrammar(grammars, index + 1, {id:id});});
        } else {
          recognizerReady();
        }
      };
      // This adds words to the recognizer. When it calls back, we add grammars
      var feedWords = function(words) {
           postRecognizerJob({command: 'addWords', data: words},
                        function() {feedGrammar(grammars, 0);});
      };

      function decode_buffer_align(decode_word, f32_arr) {
        //var i16_buf = new Int16Array(f32_arr.buffer);
        i16_buf = format_audio(f32_arr);
        console.log(f32_arr);
        console.log(i16_buf.length, i16_buf);

        //postRecognizerJob({command: 'testprint'});
        
        
        postRecognizerJob({command: 'lookupWord', data: decode_word},
              function(cbdata) {
                console.log(cbdata);
                postRecognizerJob({command: 'featex', data: {array: i16_buf, word: decode_word}});
              });
              
            
    }

    function process_stage_1(hyp_seg) {
      console.log(hyp_seg);
      var framesc = 160;
      for (var n = 1; n < hyp_seg.length - 1; n++) {
        
        // Extract tri-phone sub segment
        var left = hyp_seg[n-1].word;
        var leftn = hyp_seg[n-1].start;
        var target = hyp_seg[n].word;
        var right = hyp_seg[n+1].word;
        var rightn = hyp_seg[n+1].end;
        
        var sil = Array.apply(null, Array(16000)).map(Number.prototype.valueOf,0);
        var subseg = sil.concat(i16_buf.slice(leftn*framesc, rightn*framesc)).concat(sil);
        
      }
    }
      
    function format_audio(inputArray){
      // Convert the float samples to 16-bit integers
        var output = new Int16Array(inputArray.length);
        for (var i = 0; i < inputArray.length; i++){
            var s = Math.max(-1, Math.min(1, inputArray[i]));
            //output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            output[i] = s * 0xFFFF;
        }
        
        // Downsample audio to 16k
        console.log(inSampleRate, outputSampleRate);
        var outputBufferLength = Math.floor(output.length * outputSampleRate / inSampleRate);
        var result = new Int16Array(outputBufferLength);
        var bin = 0,
        num = 0,
        indexIn = 0,
        indexOut = 0;
      while(indexIn < outputBufferLength) {
          bin = 0;
          num = 0;
          while(indexOut < Math.min(output.length, (indexIn + 1) * inSampleRate / outputSampleRate)) {
          bin += output[indexOut];
          num += 1;
          indexOut++;
          }
          result[indexIn] = bin / num;
          indexIn++;
      }
        
        return result;
    }
      // This initializes the recognizer. When it calls back, we add words
      var initRecognizer = function() {
          // You can pass parameters to the recognizer, such as : {command: 'initialize', data: [["-hmm", "my_model"], ["-fwdflat", "no"]]}
         
          postRecognizerJob({command: 'initialize',// data: [["-hmm", "my_model"], ["-fwdflat", "no"]]}
                             //callbackId: id,
                                data: [  //["-jsgf", "wyn-align.jsgf"],
                              // ["-dict", "phonemes.dict"],
                               //  //["-hmm","my_model"],
                                ["-backtrace", "yes"],
                               ["-fsgusefiller","yes"],
                                ["-bestpath","yes"]
                                                              ]
                             },
                             
                            function(id) {
                                        if (recorder) recorder.consumers = [recognizer];
                                        feedWords(wordList);}
                                        );
      };
      // When the page is loaded, we spawn a new recognizer worker and call getUserMedia to
      // request access to the microphone
      window.onload = function() {
        outputContainer = document.getElementById("output");
        updateStatus("Initializing web audio and speech recognizer, waiting for approval to access the microphone");
        callbackManager = new CallbackManager();
        spawnWorker("static/js/recognizer.js", function(worker) {
            // This is the onmessage function, once the worker is fully loaded
            worker.onmessage = function(e) {
                if (e.data.hasOwnProperty('feats')) {
                    console.log("feats", e.data);
                    features = e.data;
                    features.word = decode_word;
                    features = JSON.stringify(features);
                    
                    int_score = ajax_post('https://tools.wmflabs.org/proneval-gsoc17/pronserv', features);
                                     
                  }

                // This is the case when we have a callback id to be called
                if (e.data.hasOwnProperty('id')) {
                  var clb = callbackManager.get(e.data['id']);
                  var data = {};
                  if ( e.data.hasOwnProperty('data')) data = e.data.data;
                  if(clb) clb(data);
                }
                // This is a case when the recognizer has a new hypothesis
                if (e.data.hasOwnProperty('hyp')) {
                  var newHyp = e.data.hyp;
                  if (e.data.hasOwnProperty('final') &&  e.data.final) newHyp =newHyp;
                  choice = newHyp;
                  if(e.data.hasOwnProperty('data')){
                    if(e.data.data.stage == 0){
                      process_stage_1(e.data.hypseg);
                    }
                  }
                }
  
                  // This is the case when we have an error
                if (e.data.hasOwnProperty('status') && (e.data.status == "error")) {
                  updateStatus("Error in " + e.data.command + " with code " + e.data.code);
                }
            };
            // Once the worker is fully loaded, we can call the initialize function
            initRecognizer();
        });
        // The following is to initialize Web Audio
        try {
          window.AudioContext = window.AudioContext || window.webkitAudioContext;
          navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
          window.URL = window.URL || window.webkitURL;
          audioContext = new AudioContext();

          inSampleRate = audioContext.sampleRate;
        } catch (e) {
          updateStatus("Error initializing Web Audio browser");
        }
        if (navigator.getUserMedia) navigator.getUserMedia({audio: true}, startUserMedia, function(e) {
                                        updateStatus("No live audio input in this browser");
                                    });
        else updateStatus("No web audio support in this browser");
      // Wiring JavaScript to the UI
      var startBtn = document.getElementById('startBtn');
      startBtn.disabled = true;
      startBtn.onclick = check;
 
      play.onclick = playback;

  // Get the button that opens the modal
      var evaluate = document.getElementById("eval");
      evaluate.onclick = feat;
      //stopBtn.onclick
      };

       // This is the list of words that need to be added to the recognizer
       // This follows the CMU dictionary format
var wordList = [["we", "W IY"], ["drank", "D R AE NG K"], ["tea", "T IY"], ["in", "IH N"], ["the", "DH AH"], ["afternoon", "AE F T ER N UW N"], ["and", "AH N D"], ["watched", "W AA CH T"], ["tv", "T IY V IY"],];

var grammarChoices = {numStates: 15, start: 0, end: 14, transitions: [{from: 0, to:1, word:"we"},{from: 1, to:2, word:"drank"},{from: 2, to:3, word:"tea"},{from: 3, to:4, word:"in"},{from: 4, to:5, word:"the"},{from: 5, to:6, word:"afternoon"},{from: 6, to:7, word:"and"},{from: 7, to:8, word:"watched"},{from: 8, to:9, word:"tv"}]};


          var grammars = [ {title: "Choices", g: grammarChoices}];
      var grammarIds = [];
    </script>
    <!-- These are the two JavaScript files you must load in the HTML,
    The recognizer is loaded through a Web Worker -->
<script src="static/js/audioRecorder.js"></script>
     <script src="static/js/callbackManager.js"></script>
     <script src="static/js/audioRecorderWorker.js"></script>
     <script src="static/js/recorder.js"></script>
     <script src="static/js/recognizer.js"></script>
     <script src="static/js/materialize.min.js"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>    
<script src="https://use.fontawesome.com/03f8396175.js"></script>
  </body>
</html>
